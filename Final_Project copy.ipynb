{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/liufamily/.cache/kagglehub/datasets/andrewsundberg/college-basketball-dataset/versions/7\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"andrewsundberg/college-basketball-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM</th>\n",
       "      <th>CONF</th>\n",
       "      <th>G</th>\n",
       "      <th>W</th>\n",
       "      <th>ADJOE</th>\n",
       "      <th>ADJDE</th>\n",
       "      <th>BARTHAG</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>EFG_D</th>\n",
       "      <th>TOR</th>\n",
       "      <th>...</th>\n",
       "      <th>FTRD</th>\n",
       "      <th>2P_O</th>\n",
       "      <th>2P_D</th>\n",
       "      <th>3P_O</th>\n",
       "      <th>3P_D</th>\n",
       "      <th>ADJ_T</th>\n",
       "      <th>WAB</th>\n",
       "      <th>POSTSEASON</th>\n",
       "      <th>SEED</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>ACC</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>123.3</td>\n",
       "      <td>94.9</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>52.6</td>\n",
       "      <td>48.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>...</td>\n",
       "      <td>30.4</td>\n",
       "      <td>53.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>32.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>71.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2ND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>B10</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>129.1</td>\n",
       "      <td>93.6</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>54.8</td>\n",
       "      <td>47.7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>...</td>\n",
       "      <td>22.4</td>\n",
       "      <td>54.8</td>\n",
       "      <td>44.7</td>\n",
       "      <td>36.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>59.3</td>\n",
       "      <td>11.3</td>\n",
       "      <td>2ND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>B10</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>114.4</td>\n",
       "      <td>90.4</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>53.9</td>\n",
       "      <td>47.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54.7</td>\n",
       "      <td>46.8</td>\n",
       "      <td>35.2</td>\n",
       "      <td>33.2</td>\n",
       "      <td>65.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2ND</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Texas Tech</td>\n",
       "      <td>B12</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>115.2</td>\n",
       "      <td>85.2</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>53.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>...</td>\n",
       "      <td>36.6</td>\n",
       "      <td>52.8</td>\n",
       "      <td>41.9</td>\n",
       "      <td>36.5</td>\n",
       "      <td>29.7</td>\n",
       "      <td>67.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2ND</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gonzaga</td>\n",
       "      <td>WCC</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>117.8</td>\n",
       "      <td>86.3</td>\n",
       "      <td>0.9728</td>\n",
       "      <td>56.6</td>\n",
       "      <td>41.1</td>\n",
       "      <td>16.2</td>\n",
       "      <td>...</td>\n",
       "      <td>26.9</td>\n",
       "      <td>56.3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>71.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2ND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>Toledo</td>\n",
       "      <td>MAC</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>119.9</td>\n",
       "      <td>109.6</td>\n",
       "      <td>0.7369</td>\n",
       "      <td>56.3</td>\n",
       "      <td>52.9</td>\n",
       "      <td>13.6</td>\n",
       "      <td>...</td>\n",
       "      <td>27.5</td>\n",
       "      <td>54.6</td>\n",
       "      <td>52.1</td>\n",
       "      <td>39.7</td>\n",
       "      <td>36.1</td>\n",
       "      <td>69.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>Liberty</td>\n",
       "      <td>ASun</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>111.4</td>\n",
       "      <td>97.3</td>\n",
       "      <td>0.8246</td>\n",
       "      <td>55.5</td>\n",
       "      <td>49.3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.8</td>\n",
       "      <td>56.4</td>\n",
       "      <td>48.6</td>\n",
       "      <td>36.4</td>\n",
       "      <td>33.6</td>\n",
       "      <td>64.4</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>Utah Valley</td>\n",
       "      <td>WAC</td>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td>107.1</td>\n",
       "      <td>94.6</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>51.7</td>\n",
       "      <td>44.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>...</td>\n",
       "      <td>28.7</td>\n",
       "      <td>52.5</td>\n",
       "      <td>42.8</td>\n",
       "      <td>33.4</td>\n",
       "      <td>31.1</td>\n",
       "      <td>69.8</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>UAB</td>\n",
       "      <td>CUSA</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>112.4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.8453</td>\n",
       "      <td>50.3</td>\n",
       "      <td>47.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>...</td>\n",
       "      <td>28.9</td>\n",
       "      <td>48.8</td>\n",
       "      <td>47.2</td>\n",
       "      <td>35.6</td>\n",
       "      <td>31.6</td>\n",
       "      <td>70.7</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>North Texas</td>\n",
       "      <td>CUSA</td>\n",
       "      <td>36</td>\n",
       "      <td>31</td>\n",
       "      <td>110.0</td>\n",
       "      <td>93.8</td>\n",
       "      <td>0.8622</td>\n",
       "      <td>51.2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>19.8</td>\n",
       "      <td>...</td>\n",
       "      <td>40.2</td>\n",
       "      <td>49.6</td>\n",
       "      <td>44.2</td>\n",
       "      <td>35.7</td>\n",
       "      <td>30.1</td>\n",
       "      <td>58.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3523 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                TEAM  CONF   G   W  ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D   TOR  \\\n",
       "0     North Carolina   ACC  40  33  123.3   94.9   0.9531   52.6   48.1  15.4   \n",
       "1          Wisconsin   B10  40  36  129.1   93.6   0.9758   54.8   47.7  12.4   \n",
       "2           Michigan   B10  40  33  114.4   90.4   0.9375   53.9   47.7  14.0   \n",
       "3         Texas Tech   B12  38  31  115.2   85.2   0.9696   53.5   43.0  17.7   \n",
       "4            Gonzaga   WCC  39  37  117.8   86.3   0.9728   56.6   41.1  16.2   \n",
       "...              ...   ...  ..  ..    ...    ...      ...    ...    ...   ...   \n",
       "3518          Toledo   MAC  34  27  119.9  109.6   0.7369   56.3   52.9  13.6   \n",
       "3519         Liberty  ASun  33  27  111.4   97.3   0.8246   55.5   49.3  16.0   \n",
       "3520     Utah Valley   WAC  34  28  107.1   94.6   0.8065   51.7   44.0  19.3   \n",
       "3521             UAB  CUSA  38  29  112.4   97.0   0.8453   50.3   47.3  17.3   \n",
       "3522     North Texas  CUSA  36  31  110.0   93.8   0.8622   51.2   44.5  19.8   \n",
       "\n",
       "      ...  FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T   WAB  POSTSEASON  SEED  YEAR  \n",
       "0     ...  30.4  53.9  44.6  32.7  36.2   71.7   8.6         2ND   1.0  2016  \n",
       "1     ...  22.4  54.8  44.7  36.5  37.5   59.3  11.3         2ND   1.0  2015  \n",
       "2     ...  30.0  54.7  46.8  35.2  33.2   65.9   6.9         2ND   3.0  2018  \n",
       "3     ...  36.6  52.8  41.9  36.5  29.7   67.5   7.0         2ND   3.0  2019  \n",
       "4     ...  26.9  56.3  40.0  38.2  29.0   71.5   7.7         2ND   1.0  2017  \n",
       "...   ...   ...   ...   ...   ...   ...    ...   ...         ...   ...   ...  \n",
       "3518  ...  27.5  54.6  52.1  39.7  36.1   69.5  -1.2         NaN   NaN  2023  \n",
       "3519  ...  27.8  56.4  48.6  36.4  33.6   64.4  -2.0         NaN   NaN  2023  \n",
       "3520  ...  28.7  52.5  42.8  33.4  31.1   69.8  -0.3         NaN   NaN  2023  \n",
       "3521  ...  28.9  48.8  47.2  35.6  31.6   70.7  -0.5         NaN   NaN  2023  \n",
       "3522  ...  40.2  49.6  44.2  35.7  30.1   58.7   1.1         NaN   NaN  2023  \n",
       "\n",
       "[3523 rows x 24 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('cbb.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/hn6y49dn3qlbbv0j58dc42tm0000gn/T/ipykernel_49694/3056234001.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_cleaned['POSTSEASON'] = data_cleaned[\"POSTSEASON\"].replace(value_mapping)\n",
      "/var/folders/q0/hn6y49dn3qlbbv0j58dc42tm0000gn/T/ipykernel_49694/3056234001.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_cleaned2['POSTSEASON'] = data_cleaned2[\"POSTSEASON\"].replace(value_mapping)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJOE</th>\n",
       "      <th>ADJDE</th>\n",
       "      <th>BARTHAG</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>EFG_D</th>\n",
       "      <th>TOR</th>\n",
       "      <th>TORD</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>FTR</th>\n",
       "      <th>...</th>\n",
       "      <th>YEAR_2013</th>\n",
       "      <th>YEAR_2014</th>\n",
       "      <th>YEAR_2015</th>\n",
       "      <th>YEAR_2016</th>\n",
       "      <th>YEAR_2017</th>\n",
       "      <th>YEAR_2018</th>\n",
       "      <th>YEAR_2019</th>\n",
       "      <th>YEAR_2021</th>\n",
       "      <th>YEAR_2022</th>\n",
       "      <th>YEAR_2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.3</td>\n",
       "      <td>94.9</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>52.6</td>\n",
       "      <td>48.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>18.2</td>\n",
       "      <td>40.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129.1</td>\n",
       "      <td>93.6</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>54.8</td>\n",
       "      <td>47.7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>15.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>23.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114.4</td>\n",
       "      <td>90.4</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>53.9</td>\n",
       "      <td>47.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>24.9</td>\n",
       "      <td>30.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.2</td>\n",
       "      <td>85.2</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>53.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>22.8</td>\n",
       "      <td>27.4</td>\n",
       "      <td>28.7</td>\n",
       "      <td>32.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.8</td>\n",
       "      <td>86.3</td>\n",
       "      <td>0.9728</td>\n",
       "      <td>56.6</td>\n",
       "      <td>41.1</td>\n",
       "      <td>16.2</td>\n",
       "      <td>17.1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>119.9</td>\n",
       "      <td>109.6</td>\n",
       "      <td>0.7369</td>\n",
       "      <td>56.3</td>\n",
       "      <td>52.9</td>\n",
       "      <td>13.6</td>\n",
       "      <td>18.3</td>\n",
       "      <td>29.8</td>\n",
       "      <td>30.9</td>\n",
       "      <td>34.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>111.4</td>\n",
       "      <td>97.3</td>\n",
       "      <td>0.8246</td>\n",
       "      <td>55.5</td>\n",
       "      <td>49.3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>25.3</td>\n",
       "      <td>20.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>107.1</td>\n",
       "      <td>94.6</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>51.7</td>\n",
       "      <td>44.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>16.3</td>\n",
       "      <td>28.9</td>\n",
       "      <td>29.1</td>\n",
       "      <td>35.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>112.4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.8453</td>\n",
       "      <td>50.3</td>\n",
       "      <td>47.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>19.3</td>\n",
       "      <td>35.8</td>\n",
       "      <td>29.3</td>\n",
       "      <td>35.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>110.0</td>\n",
       "      <td>93.8</td>\n",
       "      <td>0.8622</td>\n",
       "      <td>51.2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.9</td>\n",
       "      <td>33.4</td>\n",
       "      <td>28.6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3523 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D   TOR  TORD   ORB   DRB   FTR  ...  \\\n",
       "0     123.3   94.9   0.9531   52.6   48.1  15.4  18.2  40.7  30.0  32.3  ...   \n",
       "1     129.1   93.6   0.9758   54.8   47.7  12.4  15.8  32.1  23.7  36.2  ...   \n",
       "2     114.4   90.4   0.9375   53.9   47.7  14.0  19.5  25.5  24.9  30.7  ...   \n",
       "3     115.2   85.2   0.9696   53.5   43.0  17.7  22.8  27.4  28.7  32.9  ...   \n",
       "4     117.8   86.3   0.9728   56.6   41.1  16.2  17.1  30.0  26.2  39.0  ...   \n",
       "...     ...    ...      ...    ...    ...   ...   ...   ...   ...   ...  ...   \n",
       "3518  119.9  109.6   0.7369   56.3   52.9  13.6  18.3  29.8  30.9  34.4  ...   \n",
       "3519  111.4   97.3   0.8246   55.5   49.3  16.0  18.9  25.3  20.2  26.2  ...   \n",
       "3520  107.1   94.6   0.8065   51.7   44.0  19.3  16.3  28.9  29.1  35.6  ...   \n",
       "3521  112.4   97.0   0.8453   50.3   47.3  17.3  19.3  35.8  29.3  35.7  ...   \n",
       "3522  110.0   93.8   0.8622   51.2   44.5  19.8  19.9  33.4  28.6  31.0  ...   \n",
       "\n",
       "      YEAR_2013  YEAR_2014  YEAR_2015  YEAR_2016  YEAR_2017  YEAR_2018  \\\n",
       "0           0.0        0.0        0.0        1.0        0.0        0.0   \n",
       "1           0.0        0.0        1.0        0.0        0.0        0.0   \n",
       "2           0.0        0.0        0.0        0.0        0.0        1.0   \n",
       "3           0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4           0.0        0.0        0.0        0.0        1.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "3518        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3519        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3520        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3521        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3522        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "      YEAR_2019  YEAR_2021  YEAR_2022  YEAR_2023  \n",
       "0           0.0        0.0        0.0        0.0  \n",
       "1           0.0        0.0        0.0        0.0  \n",
       "2           0.0        0.0        0.0        0.0  \n",
       "3           1.0        0.0        0.0        0.0  \n",
       "4           0.0        0.0        0.0        0.0  \n",
       "...         ...        ...        ...        ...  \n",
       "3518        0.0        0.0        0.0        1.0  \n",
       "3519        0.0        0.0        0.0        1.0  \n",
       "3520        0.0        0.0        0.0        1.0  \n",
       "3521        0.0        0.0        0.0        1.0  \n",
       "3522        0.0        0.0        0.0        1.0  \n",
       "\n",
       "[3523 rows x 64 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_mapping = {\n",
    "    np.nan: 0,\n",
    "    \"R68\": 1,\n",
    "    \"R64\": 2,\n",
    "    \"R32\": 3,\n",
    "    \"S16\": 4,\n",
    "    \"E8\": 5,\n",
    "    \"F4\": 6,\n",
    "    \"2ND\": 7,\n",
    "    \"Champions\": 8\n",
    "}\n",
    "data_cleaned = data.copy()\n",
    "data_cleaned2 = data_cleaned.drop(['TEAM', 'G', 'W', 'CONF', 'YEAR'], axis=1)\n",
    "data_cleaned['POSTSEASON'] = data_cleaned[\"POSTSEASON\"].replace(value_mapping)\n",
    "data_cleaned2['POSTSEASON'] = data_cleaned2[\"POSTSEASON\"].replace(value_mapping)\n",
    "\n",
    "data_cleaned = pd.get_dummies(data_cleaned, columns=['CONF'])\n",
    "data_cleaned = pd.get_dummies(data_cleaned, columns=['YEAR'])\n",
    "data_cleaned = data_cleaned.drop(['TEAM', 'G', 'W'], axis=1)\n",
    "data_cleaned = data_cleaned.astype(float)\n",
    "data_cleaned2 = data_cleaned2.astype(float)\n",
    "data_cleaned2.fillna(0.0, inplace=True)\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned_no_seed = data_cleaned.drop('SEED', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_array = data_cleaned2.drop([\"POSTSEASON\", 'SEED', 'WAB'], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 7, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels_array = data_cleaned['POSTSEASON'].to_numpy()\n",
    "y_labels_array.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data_array, y_labels_array, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       450\n",
      "           1       0.60      0.88      0.71       114\n",
      "\n",
      "    accuracy                           0.86       564\n",
      "   macro avg       0.78      0.86      0.81       564\n",
      "weighted avg       0.89      0.86      0.87       564\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "y_train_binary = (y_train != 0).astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train_step1, X_test_step1, y_train_step1, y_test_step1 = train_test_split(X_train, y_train_binary, test_size=0.2)\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "log_reg.fit(X_train_step1, y_train_step1)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_step1 = log_reg.predict(X_test_step1)\n",
    "print(classification_report(y_test_step1, y_pred_step1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2254, 16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_step1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_detection(x_input, model):\n",
    "    return model.predict([x_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_step2 = X_train[y_train != 0]\n",
    "y_train_step2 = y_train[y_train != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2097 - loss: 9.6852 - val_accuracy: 0.4643 - val_loss: 3.3863\n",
      "Epoch 2/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4280 - loss: 2.8744 - val_accuracy: 0.3571 - val_loss: 1.8447\n",
      "Epoch 3/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4558 - loss: 1.8612 - val_accuracy: 0.5179 - val_loss: 1.5341\n",
      "Epoch 4/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4848 - loss: 1.7218 - val_accuracy: 0.4643 - val_loss: 1.4748\n",
      "Epoch 5/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4659 - loss: 1.6347 - val_accuracy: 0.5536 - val_loss: 1.4358\n",
      "Epoch 6/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4555 - loss: 1.4937 - val_accuracy: 0.5446 - val_loss: 1.4306\n",
      "Epoch 7/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4899 - loss: 1.4756 - val_accuracy: 0.5625 - val_loss: 1.4584\n",
      "Epoch 8/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3808 - loss: 1.6195 - val_accuracy: 0.5625 - val_loss: 1.4485\n",
      "Epoch 9/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4589 - loss: 1.5073 - val_accuracy: 0.5625 - val_loss: 1.3800\n",
      "Epoch 10/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4961 - loss: 1.3579 - val_accuracy: 0.5446 - val_loss: 1.3903\n",
      "Epoch 11/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4998 - loss: 1.4890 - val_accuracy: 0.5268 - val_loss: 1.4342\n",
      "Epoch 12/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4650 - loss: 1.4373 - val_accuracy: 0.5625 - val_loss: 1.4023\n",
      "Epoch 13/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4860 - loss: 1.5058 - val_accuracy: 0.5714 - val_loss: 1.4073\n",
      "Epoch 14/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4880 - loss: 1.4312 - val_accuracy: 0.5446 - val_loss: 1.3907\n",
      "Epoch 15/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4696 - loss: 1.4023 - val_accuracy: 0.5893 - val_loss: 1.3231\n",
      "Epoch 16/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5099 - loss: 1.4150 - val_accuracy: 0.5446 - val_loss: 1.3688\n",
      "Epoch 17/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4643 - loss: 1.4886 - val_accuracy: 0.5446 - val_loss: 1.3448\n",
      "Epoch 18/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4889 - loss: 1.3823 - val_accuracy: 0.5893 - val_loss: 1.2863\n",
      "Epoch 19/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4534 - loss: 1.5232 - val_accuracy: 0.5446 - val_loss: 1.7039\n",
      "Epoch 20/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4865 - loss: 1.5381 - val_accuracy: 0.4732 - val_loss: 1.4737\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3858 - loss: 1.6426 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6503852605819702, 0.3629032373428345]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot = tf.one_hot(y_train_step2 - 1, depth=8)  # Classes 1-8 become indices 0-7\n",
    "y_test_one_hot = tf.one_hot(y_test[y_test != 0] - 1, depth=8)\n",
    "\n",
    "# Define the neural network\n",
    "input_dim = 16  # Number of features in your input data\n",
    "nn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(input_dim,)),  # Input layer with 16 features\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='softmax')  # 8 output classes (1-8)\n",
    "])\n",
    "\n",
    "# Compile the nn_model\n",
    "nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the nn_model\n",
    "nn_model.fit(X_train_step2, y_train_one_hot, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the nn_model\n",
    "nn_model.evaluate(X_test[y_test != 0], y_test_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.47580645161290325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Filter non-class 0 data\n",
    "X_train_step2 = X_train[y_train != 0]\n",
    "y_train_step2 = y_train[y_train != 0]\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "rf.fit(X_train_step2, y_train_step2)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_rf = rf.predict(X_test[y_test != 0])\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test[y_test != 0], y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.24193548387096775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM (with probability estimates enabled)\n",
    "svm = SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "svm.fit(X_train_step2, y_train_step2)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_svm = svm.predict(X_test[y_test != 0])\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test[y_test != 0], y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_classifier(x_input, anomaly_model, multiclass_models, model):\n",
    "    is_anomalous = anomaly_detection(x_input, anomaly_model)\n",
    "    if is_anomalous == 0:\n",
    "        return 0  # Class 0\n",
    "    nn_pred = multiclass_models['nn'].predict([x_input]).argmax()\n",
    "    svm_pred = multiclass_models['svm'].predict([x_input]).argmax()\n",
    "    rf_pred = multiclass_models['rf'].predict([x_input]).argmax()\n",
    "    return multiclass_models[model].predict([x_input]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_models = {\n",
    "    'rf': rf,  # Random Forest\n",
    "    'svm': svm,  # Support Vector Machine\n",
    "    'nn': nn_model,   # Neural Network\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Evaluating Step 1 (Anomaly Detection) ###\n",
      "Step 1: Anomaly Detection Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       581\n",
      "           1       0.50      0.85      0.63       124\n",
      "\n",
      "    accuracy                           0.82       705\n",
      "   macro avg       0.73      0.84      0.76       705\n",
      "weighted avg       0.88      0.82      0.84       705\n",
      "\n",
      "\n",
      "### Evaluating Step 2 (Multiclass Classification) SVM ###\n",
      "Step 2: Multiclass Classification Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.14      0.43      0.21         7\n",
      "         2.0       0.65      0.22      0.33        50\n",
      "         3.0       0.44      0.22      0.30        36\n",
      "         4.0       0.00      0.00      0.00        12\n",
      "         5.0       0.12      0.60      0.21        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.12      0.67      0.21         3\n",
      "\n",
      "    accuracy                           0.24       124\n",
      "   macro avg       0.19      0.27      0.16       124\n",
      "weighted avg       0.41      0.24      0.25       124\n",
      "\n",
      "\n",
      "### Evaluating Step 2 (Multiclass Classification) RF ###\n",
      "Step 2: Multiclass Classification Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         7\n",
      "         2.0       0.53      0.80      0.64        50\n",
      "         3.0       0.54      0.36      0.43        36\n",
      "         4.0       0.26      0.42      0.32        12\n",
      "         5.0       0.00      0.00      0.00        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.48       124\n",
      "   macro avg       0.23      0.24      0.22       124\n",
      "weighted avg       0.41      0.48      0.42       124\n",
      "\n",
      "\n",
      "### Evaluating Step 2 (Multiclass Classification) NN ###\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Multiclass Classification Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.11      0.71      0.19         7\n",
      "         2.0       0.40      0.12      0.18        50\n",
      "         3.0       0.34      0.58      0.43        36\n",
      "         4.0       0.00      0.00      0.00        12\n",
      "         5.0       0.00      0.00      0.00        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.26       124\n",
      "   macro avg       0.11      0.18      0.10       124\n",
      "weighted avg       0.27      0.26      0.21       124\n",
      "\n",
      "\n",
      "### Evaluating Full Hierarchical Classifier SVM ###\n",
      "Full Pipeline Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.82      0.88       581\n",
      "         1.0       0.00      0.00      0.00         7\n",
      "         2.0       0.11      0.18      0.14        50\n",
      "         3.0       0.21      0.22      0.22        36\n",
      "         4.0       0.00      0.00      0.00        12\n",
      "         5.0       0.12      0.60      0.20        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.12      0.67      0.21         3\n",
      "\n",
      "    accuracy                           0.71       705\n",
      "   macro avg       0.17      0.28      0.18       705\n",
      "weighted avg       0.81      0.71      0.75       705\n",
      "\n",
      "Confusion Matrix:\n",
      "[[475  15  68  20   0   2   0   1   0]\n",
      " [  3   0   2   0   0   2   0   0   0]\n",
      " [ 12   6   9   9   0  12   0   1   1]\n",
      " [  2   0   4   8   0  17   0   0   5]\n",
      " [  0   0   0   1   0   8   0   2   1]\n",
      " [  1   0   0   0   0   6   0   0   3]\n",
      " [  0   0   0   0   0   2   0   0   2]\n",
      " [  0   0   0   0   0   0   0   0   2]\n",
      " [  0   0   0   0   0   1   0   0   2]]\n",
      "\n",
      "### Evaluating Full Hierarchical Classifier RF ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Pipeline Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.82      0.88       581\n",
      "         1.0       0.00      0.00      0.00         7\n",
      "         2.0       0.17      0.56      0.27        50\n",
      "         3.0       0.48      0.36      0.41        36\n",
      "         4.0       0.26      0.42      0.32        12\n",
      "         5.0       0.00      0.00      0.00        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.74       705\n",
      "   macro avg       0.26      0.28      0.25       705\n",
      "weighted avg       0.84      0.74      0.78       705\n",
      "\n",
      "Confusion Matrix:\n",
      "[[475   0 103   3   0   0   0   0   0]\n",
      " [  3   0   3   1   0   0   0   0   0]\n",
      " [ 12   0  28   7   3   0   0   0   0]\n",
      " [  2   0  16  13   3   2   0   0   0]\n",
      " [  0   0   6   1   5   0   0   0   0]\n",
      " [  1   0   3   2   4   0   0   0   0]\n",
      " [  0   0   1   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0   1]\n",
      " [  0   0   0   0   1   1   0   0   1]]\n",
      "\n",
      "### Evaluating Full Hierarchical Classifier NN ###\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Full Pipeline Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.82      0.88       581\n",
      "         1.0       0.02      0.29      0.03         7\n",
      "         2.0       0.28      0.10      0.15        50\n",
      "         3.0       0.25      0.58      0.35        36\n",
      "         4.0       0.00      0.00      0.00        12\n",
      "         5.0       0.00      0.00      0.00        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.71       705\n",
      "   macro avg       0.17      0.20      0.16       705\n",
      "weighted avg       0.83      0.71      0.76       705\n",
      "\n",
      "Confusion Matrix:\n",
      "[[475  81   4  21   0   0   0   0   0]\n",
      " [  3   2   1   1   0   0   0   0   0]\n",
      " [ 12  18   5  15   0   0   0   0   0]\n",
      " [  2   7   6  21   0   0   0   0   0]\n",
      " [  0   1   1  10   0   0   0   0   0]\n",
      " [  1   0   1   8   0   0   0   0   0]\n",
      " [  0   1   0   3   0   0   0   0   0]\n",
      " [  0   0   0   1   1   0   0   0   0]\n",
      " [  0   0   0   3   0   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Anomaly Detection\n",
    "def anomaly_detection(x_input, log_reg):\n",
    "    \"\"\"Predict if input belongs to class 0 or classes 1-8 using the logistic regression model.\"\"\"\n",
    "    return log_reg.predict([x_input])[0]  # 0: normal (class 0), 1: anomaly (classes 1-8)\n",
    "\n",
    "# Step 2: Multiclass Classification\n",
    "def multiclass_classification(x_input, multiclass_models, model):\n",
    "    \"\"\"Classify input among classes 1-8 using a chosen model.\"\"\"\n",
    "    if model == 'nn': \n",
    "        x_input = np.expand_dims(x_input, axis=0) \n",
    "        y_pred_probs = multiclass_models['nn'].predict(x_input)\n",
    "        return y_pred_probs.argmax() \n",
    "    else:  # Random Forest or SVM\n",
    "        return multiclass_models[model].predict([x_input])[0]\n",
    "\n",
    "# Hierarchical Classifier\n",
    "def hierarchical_classifier(x_input, log_reg, multiclass_models, model):\n",
    "    \"\"\"Hierarchical classifier combining step 1 and step 2.\"\"\"\n",
    "    is_anomalous = anomaly_detection(x_input, log_reg)\n",
    "    if is_anomalous == 0:\n",
    "        return 0  # Class 0\n",
    "    return multiclass_classification(x_input, multiclass_models, model)\n",
    "\n",
    "# Evaluate Step 1: Anomaly Detection\n",
    "def evaluate_step1(log_reg, X_test, y_test):\n",
    "    \"\"\"Evaluate anomaly detection model.\"\"\"\n",
    "    y_test_step1 = (y_test != 0).astype(int)  # Binary labels: 0 (normal) or 1 (anomaly)\n",
    "    y_pred_step1 = log_reg.predict(X_test)\n",
    "    print(\"Step 1: Anomaly Detection Metrics\")\n",
    "    print(classification_report(y_test_step1, y_pred_step1))\n",
    "    return y_pred_step1\n",
    "\n",
    "# Evaluate Step 2: Multiclass Classification\n",
    "def evaluate_step2(multiclass_models, X_test, y_test, model):\n",
    "    \"\"\"Evaluate multiclass classification model.\"\"\"\n",
    "    X_test_step2 = X_test[y_test != 0]  # Only anomalies\n",
    "    y_test_step2 = y_test[y_test != 0]  # True labels for anomalies\n",
    "    if model == 'nn':  # For Neural Network\n",
    "        # Ensure input is 2D\n",
    "        if X_test_step2.ndim == 1:\n",
    "            X_test_step2 = np.expand_dims(X_test_step2, axis=0)  # Shape: (1, n_features)\n",
    "        y_pred_probs = multiclass_models['nn'].predict(X_test_step2)\n",
    "        y_pred_step2 = y_pred_probs.argmax(axis=1)  # Get class labels from probabilities\n",
    "    else:  # For Random Forest or SVM\n",
    "        y_pred_step2 = multiclass_models[model].predict(X_test_step2)\n",
    "    print(\"Step 2: Multiclass Classification Metrics\")\n",
    "    print(classification_report(y_test_step2, y_pred_step2))\n",
    "    return y_pred_step2\n",
    "\n",
    "# Evaluate Full Pipeline\n",
    "def evaluate_pipeline(X_test, y_test, log_reg, multiclass_models, model):\n",
    "    \"\"\"Evaluate the full hierarchical classifier pipeline.\"\"\"\n",
    "    y_pred = []\n",
    "    for x_input in X_test:\n",
    "        y_pred.append(hierarchical_classifier(x_input, log_reg, multiclass_models, model))\n",
    "    \n",
    "    print(\"Full Pipeline Metrics\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Example Usage\n",
    "# Assumptions:\n",
    "# - `log_reg`: Trained logistic regression model for step 1\n",
    "# - `multiclass_models`: Dictionary containing trained models for step 2\n",
    "#   E.g., {'rf': RandomForestClassifier, 'svm': SVM, 'nn': NeuralNetwork}\n",
    "# - `X_test`, `y_test`: Full test dataset and labels\n",
    "\n",
    "# Evaluate Step 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Evaluating Step 1 (Anomaly Detection) ###\n",
      "Step 1: Anomaly Detection Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       581\n",
      "           1       0.50      0.85      0.63       124\n",
      "\n",
      "    accuracy                           0.82       705\n",
      "   macro avg       0.73      0.84      0.76       705\n",
      "weighted avg       0.88      0.82      0.84       705\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"### Evaluating Step 1 (Anomaly Detection) ###\")\n",
    "evaluate_step1(log_reg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Evaluating Step 2 (Multiclass Classification) SVM ###\n",
      "Step 2: Multiclass Classification Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.14      0.43      0.21         7\n",
      "         2.0       0.65      0.22      0.33        50\n",
      "         3.0       0.44      0.22      0.30        36\n",
      "         4.0       0.00      0.00      0.00        12\n",
      "         5.0       0.12      0.60      0.21        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.12      0.67      0.21         3\n",
      "\n",
      "    accuracy                           0.24       124\n",
      "   macro avg       0.19      0.27      0.16       124\n",
      "weighted avg       0.41      0.24      0.25       124\n",
      "\n",
      "\n",
      "### Evaluating Step 2 (Multiclass Classification) RF ###\n",
      "Step 2: Multiclass Classification Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         7\n",
      "         2.0       0.53      0.80      0.64        50\n",
      "         3.0       0.54      0.36      0.43        36\n",
      "         4.0       0.26      0.42      0.32        12\n",
      "         5.0       0.00      0.00      0.00        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.48       124\n",
      "   macro avg       0.23      0.24      0.22       124\n",
      "weighted avg       0.41      0.48      0.42       124\n",
      "\n",
      "\n",
      "### Evaluating Step 2 (Multiclass Classification) NN ###\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Multiclass Classification Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.11      0.71      0.19         7\n",
      "         2.0       0.40      0.12      0.18        50\n",
      "         3.0       0.34      0.58      0.43        36\n",
      "         4.0       0.00      0.00      0.00        12\n",
      "         5.0       0.00      0.00      0.00        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.26       124\n",
      "   macro avg       0.11      0.18      0.10       124\n",
      "weighted avg       0.27      0.26      0.21       124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 2, 3, 1, 3, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 3, 1, 3, 1, 3,\n",
       "       3, 3, 1, 3, 1, 1, 1, 1, 1, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 2, 1, 3,\n",
       "       3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 1, 3, 1, 1, 2, 1, 1, 3, 1, 3, 3, 1,\n",
       "       3, 1, 3, 2, 3, 3, 1, 3, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3, 1, 1, 3, 3,\n",
       "       3, 3, 1, 2, 1, 3, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 2,\n",
       "       2, 2, 1, 1, 1, 3, 3, 4, 2, 1, 3, 3, 1, 3])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n### Evaluating Step 2 (Multiclass Classification) SVM ###\")\n",
    "evaluate_step2(multiclass_models, X_test, y_test, 'svm')\n",
    "print(\"\\n### Evaluating Step 2 (Multiclass Classification) RF ###\")\n",
    "evaluate_step2(multiclass_models, X_test, y_test, 'rf')\n",
    "print(\"\\n### Evaluating Step 2 (Multiclass Classification) NN ###\")\n",
    "evaluate_step2(multiclass_models, X_test, y_test, 'nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Evaluating Full Hierarchical Classifier SVM ###\n",
      "Full Pipeline Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.82      0.88       581\n",
      "         1.0       0.00      0.00      0.00         7\n",
      "         2.0       0.11      0.18      0.14        50\n",
      "         3.0       0.21      0.22      0.22        36\n",
      "         4.0       0.00      0.00      0.00        12\n",
      "         5.0       0.12      0.60      0.20        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.12      0.67      0.21         3\n",
      "\n",
      "    accuracy                           0.71       705\n",
      "   macro avg       0.17      0.28      0.18       705\n",
      "weighted avg       0.81      0.71      0.75       705\n",
      "\n",
      "Confusion Matrix:\n",
      "[[475  15  68  20   0   2   0   1   0]\n",
      " [  3   0   2   0   0   2   0   0   0]\n",
      " [ 12   6   9   9   0  12   0   1   1]\n",
      " [  2   0   4   8   0  17   0   0   5]\n",
      " [  0   0   0   1   0   8   0   2   1]\n",
      " [  1   0   0   0   0   6   0   0   3]\n",
      " [  0   0   0   0   0   2   0   0   2]\n",
      " [  0   0   0   0   0   0   0   0   2]\n",
      " [  0   0   0   0   0   1   0   0   2]]\n",
      "\n",
      "### Evaluating Full Hierarchical Classifier RF ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Pipeline Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.82      0.88       581\n",
      "         1.0       0.00      0.00      0.00         7\n",
      "         2.0       0.17      0.56      0.27        50\n",
      "         3.0       0.48      0.36      0.41        36\n",
      "         4.0       0.26      0.42      0.32        12\n",
      "         5.0       0.00      0.00      0.00        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.74       705\n",
      "   macro avg       0.26      0.28      0.25       705\n",
      "weighted avg       0.84      0.74      0.78       705\n",
      "\n",
      "Confusion Matrix:\n",
      "[[475   0 103   3   0   0   0   0   0]\n",
      " [  3   0   3   1   0   0   0   0   0]\n",
      " [ 12   0  28   7   3   0   0   0   0]\n",
      " [  2   0  16  13   3   2   0   0   0]\n",
      " [  0   0   6   1   5   0   0   0   0]\n",
      " [  1   0   3   2   4   0   0   0   0]\n",
      " [  0   0   1   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0   1]\n",
      " [  0   0   0   0   1   1   0   0   1]]\n",
      "\n",
      "### Evaluating Full Hierarchical Classifier NN ###\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Full Pipeline Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.82      0.88       581\n",
      "         1.0       0.02      0.29      0.03         7\n",
      "         2.0       0.28      0.10      0.15        50\n",
      "         3.0       0.25      0.58      0.35        36\n",
      "         4.0       0.00      0.00      0.00        12\n",
      "         5.0       0.00      0.00      0.00        10\n",
      "         6.0       0.00      0.00      0.00         4\n",
      "         7.0       0.00      0.00      0.00         2\n",
      "         8.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.71       705\n",
      "   macro avg       0.17      0.20      0.16       705\n",
      "weighted avg       0.83      0.71      0.76       705\n",
      "\n",
      "Confusion Matrix:\n",
      "[[475  81   4  21   0   0   0   0   0]\n",
      " [  3   2   1   1   0   0   0   0   0]\n",
      " [ 12  18   5  15   0   0   0   0   0]\n",
      " [  2   7   6  21   0   0   0   0   0]\n",
      " [  0   1   1  10   0   0   0   0   0]\n",
      " [  1   0   1   8   0   0   0   0   0]\n",
      " [  0   1   0   3   0   0   0   0   0]\n",
      " [  0   0   0   1   1   0   0   0   0]\n",
      " [  0   0   0   3   0   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Full Pipeline\n",
    "print(\"\\n### Evaluating Full Hierarchical Classifier SVM ###\")\n",
    "evaluate_pipeline(X_test, y_test, log_reg, multiclass_models, 'svm')\n",
    "print(\"\\n### Evaluating Full Hierarchical Classifier RF ###\")\n",
    "evaluate_pipeline(X_test, y_test, log_reg, multiclass_models, 'rf')\n",
    "print(\"\\n### Evaluating Full Hierarchical Classifier NN ###\")\n",
    "evaluate_pipeline(X_test, y_test, log_reg, multiclass_models, 'nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance(rf_model, feature_names=None):\n",
    "    \"\"\"\n",
    "    Plot feature importances for a trained Random Forest model.\n",
    "    \n",
    "    Args:\n",
    "        rf_model (RandomForestClassifier): Trained Random Forest model.\n",
    "        feature_names (list, optional): List of feature names. If None, features will be named generically.\n",
    "    \"\"\"\n",
    "    # Get feature importances\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]  # Sort features by importance (descending order)\n",
    "\n",
    "    # Feature names\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature {i}\" for i in range(len(importances))]\n",
    "\n",
    "    # Sort feature names by importance\n",
    "    sorted_feature_names = np.array(feature_names)[indices]\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(len(importances)), sorted_feature_names, rotation=45, ha=\"right\")\n",
    "    plt.title(\"Feature Importance from Random Forest\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Importance Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "trained_rf_model = multiclass_models['rf']  # Your trained Random Forest model\n",
    "feature_names = [f\"Feature {i+1}\" for i in range(X_test.shape[1])]  # Adjust based on your features\n",
    "plot_feature_importance(trained_rf_model, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJOE</th>\n",
       "      <th>ADJDE</th>\n",
       "      <th>BARTHAG</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>EFG_D</th>\n",
       "      <th>TOR</th>\n",
       "      <th>TORD</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>FTR</th>\n",
       "      <th>FTRD</th>\n",
       "      <th>2P_O</th>\n",
       "      <th>2P_D</th>\n",
       "      <th>3P_O</th>\n",
       "      <th>3P_D</th>\n",
       "      <th>ADJ_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.3</td>\n",
       "      <td>94.9</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>52.6</td>\n",
       "      <td>48.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>18.2</td>\n",
       "      <td>40.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.3</td>\n",
       "      <td>30.4</td>\n",
       "      <td>53.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>32.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>71.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129.1</td>\n",
       "      <td>93.6</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>54.8</td>\n",
       "      <td>47.7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>15.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>23.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>22.4</td>\n",
       "      <td>54.8</td>\n",
       "      <td>44.7</td>\n",
       "      <td>36.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>59.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114.4</td>\n",
       "      <td>90.4</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>53.9</td>\n",
       "      <td>47.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>24.9</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54.7</td>\n",
       "      <td>46.8</td>\n",
       "      <td>35.2</td>\n",
       "      <td>33.2</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.2</td>\n",
       "      <td>85.2</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>53.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>22.8</td>\n",
       "      <td>27.4</td>\n",
       "      <td>28.7</td>\n",
       "      <td>32.9</td>\n",
       "      <td>36.6</td>\n",
       "      <td>52.8</td>\n",
       "      <td>41.9</td>\n",
       "      <td>36.5</td>\n",
       "      <td>29.7</td>\n",
       "      <td>67.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.8</td>\n",
       "      <td>86.3</td>\n",
       "      <td>0.9728</td>\n",
       "      <td>56.6</td>\n",
       "      <td>41.1</td>\n",
       "      <td>16.2</td>\n",
       "      <td>17.1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>56.3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>71.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>119.9</td>\n",
       "      <td>109.6</td>\n",
       "      <td>0.7369</td>\n",
       "      <td>56.3</td>\n",
       "      <td>52.9</td>\n",
       "      <td>13.6</td>\n",
       "      <td>18.3</td>\n",
       "      <td>29.8</td>\n",
       "      <td>30.9</td>\n",
       "      <td>34.4</td>\n",
       "      <td>27.5</td>\n",
       "      <td>54.6</td>\n",
       "      <td>52.1</td>\n",
       "      <td>39.7</td>\n",
       "      <td>36.1</td>\n",
       "      <td>69.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>111.4</td>\n",
       "      <td>97.3</td>\n",
       "      <td>0.8246</td>\n",
       "      <td>55.5</td>\n",
       "      <td>49.3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>25.3</td>\n",
       "      <td>20.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>27.8</td>\n",
       "      <td>56.4</td>\n",
       "      <td>48.6</td>\n",
       "      <td>36.4</td>\n",
       "      <td>33.6</td>\n",
       "      <td>64.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>107.1</td>\n",
       "      <td>94.6</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>51.7</td>\n",
       "      <td>44.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>16.3</td>\n",
       "      <td>28.9</td>\n",
       "      <td>29.1</td>\n",
       "      <td>35.6</td>\n",
       "      <td>28.7</td>\n",
       "      <td>52.5</td>\n",
       "      <td>42.8</td>\n",
       "      <td>33.4</td>\n",
       "      <td>31.1</td>\n",
       "      <td>69.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>112.4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.8453</td>\n",
       "      <td>50.3</td>\n",
       "      <td>47.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>19.3</td>\n",
       "      <td>35.8</td>\n",
       "      <td>29.3</td>\n",
       "      <td>35.7</td>\n",
       "      <td>28.9</td>\n",
       "      <td>48.8</td>\n",
       "      <td>47.2</td>\n",
       "      <td>35.6</td>\n",
       "      <td>31.6</td>\n",
       "      <td>70.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>110.0</td>\n",
       "      <td>93.8</td>\n",
       "      <td>0.8622</td>\n",
       "      <td>51.2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.9</td>\n",
       "      <td>33.4</td>\n",
       "      <td>28.6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>40.2</td>\n",
       "      <td>49.6</td>\n",
       "      <td>44.2</td>\n",
       "      <td>35.7</td>\n",
       "      <td>30.1</td>\n",
       "      <td>58.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3523 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D   TOR  TORD   ORB   DRB   FTR  FTRD  \\\n",
       "0     123.3   94.9   0.9531   52.6   48.1  15.4  18.2  40.7  30.0  32.3  30.4   \n",
       "1     129.1   93.6   0.9758   54.8   47.7  12.4  15.8  32.1  23.7  36.2  22.4   \n",
       "2     114.4   90.4   0.9375   53.9   47.7  14.0  19.5  25.5  24.9  30.7  30.0   \n",
       "3     115.2   85.2   0.9696   53.5   43.0  17.7  22.8  27.4  28.7  32.9  36.6   \n",
       "4     117.8   86.3   0.9728   56.6   41.1  16.2  17.1  30.0  26.2  39.0  26.9   \n",
       "...     ...    ...      ...    ...    ...   ...   ...   ...   ...   ...   ...   \n",
       "3518  119.9  109.6   0.7369   56.3   52.9  13.6  18.3  29.8  30.9  34.4  27.5   \n",
       "3519  111.4   97.3   0.8246   55.5   49.3  16.0  18.9  25.3  20.2  26.2  27.8   \n",
       "3520  107.1   94.6   0.8065   51.7   44.0  19.3  16.3  28.9  29.1  35.6  28.7   \n",
       "3521  112.4   97.0   0.8453   50.3   47.3  17.3  19.3  35.8  29.3  35.7  28.9   \n",
       "3522  110.0   93.8   0.8622   51.2   44.5  19.8  19.9  33.4  28.6  31.0  40.2   \n",
       "\n",
       "      2P_O  2P_D  3P_O  3P_D  ADJ_T  \n",
       "0     53.9  44.6  32.7  36.2   71.7  \n",
       "1     54.8  44.7  36.5  37.5   59.3  \n",
       "2     54.7  46.8  35.2  33.2   65.9  \n",
       "3     52.8  41.9  36.5  29.7   67.5  \n",
       "4     56.3  40.0  38.2  29.0   71.5  \n",
       "...    ...   ...   ...   ...    ...  \n",
       "3518  54.6  52.1  39.7  36.1   69.5  \n",
       "3519  56.4  48.6  36.4  33.6   64.4  \n",
       "3520  52.5  42.8  33.4  31.1   69.8  \n",
       "3521  48.8  47.2  35.6  31.6   70.7  \n",
       "3522  49.6  44.2  35.7  30.1   58.7  \n",
       "\n",
       "[3523 rows x 16 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned2.drop([\"POSTSEASON\", 'SEED', 'WAB'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pulp\n",
      "  Downloading PuLP-2.9.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Downloading PuLP-2.9.0-py3-none-any.whl (17.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pulp\n",
      "\u001b[33m  WARNING: The script pulptest is installed in '/Library/Frameworks/Python.framework/Versions/3.11/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pulp-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/q0/hn6y49dn3qlbbv0j58dc42tm0000gn/T/27670ead2ff045eb90f03479f9a4a6ba-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /var/folders/q0/hn6y49dn3qlbbv0j58dc42tm0000gn/T/27670ead2ff045eb90f03479f9a4a6ba-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 15 COLUMNS\n",
      "At line 98 RHS\n",
      "At line 109 BOUNDS\n",
      "At line 126 ENDATA\n",
      "Problem MODEL has 10 rows, 16 columns and 30 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0 - 0.00 seconds\n",
      "Cgl0004I processed model has 0 rows, 0 columns (0 integer (0 of which binary)) and 0 elements\n",
      "Cbc3007W No integer variables - nothing to do\n",
      "Cuts at root node changed objective from 0 to -1.79769e+308\n",
      "Probing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.00000000\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.00\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.01\n",
      "\n",
      "Optimal orders: {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.0, (0, 3): 0.0, (1, 0): 0.0, (1, 1): 0.0, (1, 2): 0.0, (2, 0): 0.0, (2, 1): 0.0, (3, 0): 0.0}\n",
      "Total Revenue: 0.0\n",
      "Total Cost: 0.0\n",
      "Profit: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "\n",
    "# Define states and actions\n",
    "states = [0, 1, 2, 3]\n",
    "actions = {\n",
    "    0: [0, 1, 2, 3],\n",
    "    1: [0, 1, 2],\n",
    "    2: [0, 1],\n",
    "    3: [0],\n",
    "}\n",
    "\n",
    "# Transition probabilities P[i][a][j]\n",
    "P = {\n",
    "    0: {\n",
    "        0: {0: 1},                   # P000 = 1\n",
    "        1: {1: 0.2, 0: 0.8},         # P011 = 0.2, P010 = 0.9\n",
    "        2: {2: 0.2, 1: 0.2, 0: 0.6},\n",
    "        3: {3: 0.2, 2: 0.2, 1: 0.5, 0: 0.1},\n",
    "    },\n",
    "    1: {\n",
    "        0: {0: 0.2, 1: 0.8},         # P100 = 0.2, P101 = 0.8\n",
    "        1: {1: 0.2, 2: 0.2, 0: 0.6}, # P111 = 0.7, P112 = 0.3\n",
    "        2: {3: 0.2, 2: 0.2, 1: 0.5, 0: 0.1},\n",
    "    },\n",
    "    2: {\n",
    "        0: {3: 0.2, 2: 0.2, 1: 0.6}, # P200 = 0.2, P201 = 0.7, P202 = 0.1\n",
    "        1: {3: 0.2, 2: 0.2, 1: 0.5, 0: 0.1},\n",
    "    },\n",
    "    3: {\n",
    "        0: {3: 0.2, 2: 0.2, 1: 0.5, 0: 0.1},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Costs c(i, a)\n",
    "c = {\n",
    "    (3, 0): 6,\n",
    "    (2, 1): 61,\n",
    "    (1, 2): 81,\n",
    "    (0, 3): 101,\n",
    "    (2, 0): 3.9,\n",
    "    (1, 1): 58.9,\n",
    "    (0, 2): 78.9,\n",
    "    (0, 0): 22.5,\n",
    "    (0, 1): 66.3,\n",
    "    (1, 0): 11.3,\n",
    "}\n",
    "\n",
    "# Revenue per item sold\n",
    "REVENUE_PER_ITEM = 250\n",
    "\n",
    "# Create the Linear Program (LP) problem\n",
    "prob = pulp.LpProblem(\"Minimize_Cost_while_Maximizing_Profit\", pulp.LpMinimize)\n",
    "\n",
    "# Decision Variables: how many items to order in each state-action pair\n",
    "x = pulp.LpVariable.dicts(\"Order\", (states, actions), lowBound=0, cat='Integer')\n",
    "\n",
    "# Objective: Minimize costs (including ordering and holding costs)\n",
    "prob += pulp.lpSum(\n",
    "    c[s, a] * x[s][a] for s in states for a in actions[s]\n",
    "), \"Total_Cost\"\n",
    "\n",
    "# Add constraints for transition flow (based on transition probabilities)\n",
    "for s in states:\n",
    "    for a in actions[s]:\n",
    "        prob += pulp.lpSum(x[s][a] * P[s][a].get(s_prime, 0) for s_prime in states) == pulp.lpSum(x[s_prime][a] for s_prime in states), f\"Transition_{s}_{a}\"\n",
    "\n",
    "# Solve the LP\n",
    "prob.solve()\n",
    "\n",
    "# Extract the optimal ordering quantities\n",
    "optimal_orders = {}\n",
    "for s in states:\n",
    "    for a in actions[s]:\n",
    "        optimal_orders[(s, a)] = pulp.value(x[s][a])\n",
    "\n",
    "# Calculate profit based on revenue\n",
    "total_revenue = 0\n",
    "total_cost = pulp.value(prob.objective)  # The minimized cost\n",
    "\n",
    "# Assuming that the items ordered are sold at the price of 250\n",
    "for s in states:\n",
    "    for a in actions[s]:\n",
    "        ordered_items = optimal_orders[(s, a)]\n",
    "        total_revenue += ordered_items * REVENUE_PER_ITEM\n",
    "\n",
    "# Calculate profit (Profit = Revenue - Total Cost)\n",
    "profit = total_revenue - total_cost\n",
    "\n",
    "# Print the results\n",
    "print(f\"Optimal orders: {optimal_orders}\")\n",
    "print(f\"Total Revenue: {total_revenue}\")\n",
    "print(f\"Total Cost: {total_cost}\")\n",
    "print(f\"Profit: {profit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/q0/hn6y49dn3qlbbv0j58dc42tm0000gn/T/1442f8a80dce47319d9b5f4423c5fc9d-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /var/folders/q0/hn6y49dn3qlbbv0j58dc42tm0000gn/T/1442f8a80dce47319d9b5f4423c5fc9d-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 10 COLUMNS\n",
      "At line 61 RHS\n",
      "At line 67 BOUNDS\n",
      "At line 68 ENDATA\n",
      "Problem MODEL has 5 rows, 10 columns and 39 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 5 (0) rows, 10 (0) columns and 39 (0) elements\n",
      "0  Obj 0 Primal inf 0.9999999 (1)\n",
      "4  Obj 33.211111\n",
      "Optimal - objective value 33.211111\n",
      "Optimal objective 33.21111111 - 4 iterations time 0.002\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.01\n",
      "\n",
      "Status: Optimal\n",
      "Optimal z(i,a) values:\n",
      "z(0, 0) = 0.0\n",
      "z(0, 1) = 0.0\n",
      "z(0, 2) = 0.0\n",
      "z(0, 3) = 0.16666667\n",
      "z(1, 0) = 0.72222222\n",
      "z(1, 1) = 0.0\n",
      "z(1, 2) = 0.0\n",
      "z(2, 0) = 0.055555556\n",
      "z(2, 1) = 0.0\n",
      "z(3, 0) = 0.055555556\n",
      "\n",
      "Optimal Policy f_{i,a}:\n",
      "f(0,0) = 0.0000\n",
      "f(0,1) = 0.0000\n",
      "f(0,2) = 0.0000\n",
      "f(0,3) = 1.0000\n",
      "f(1,0) = 1.0000\n",
      "f(1,1) = 0.0000\n",
      "f(1,2) = 0.0000\n",
      "f(2,0) = 1.0000\n",
      "f(2,1) = 0.0000\n",
      "f(3,0) = 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "\n",
    "# Input Data\n",
    "states = [0, 1, 2, 3]  # State space S\n",
    "actions = {\n",
    "    0: [0, 1, 2, 3],\n",
    "    1: [0, 1, 2],\n",
    "    2: [0, 1],\n",
    "    3: [0],\n",
    "}  # Action space A_i\n",
    "\n",
    "# Transition probabilities P[i][a][j]\n",
    "P = {\n",
    "    0: {\n",
    "        0: {0: 1},\n",
    "        1: {1: 0.2, 0: 0.8},\n",
    "        2: {2: 0.2, 1: 0.2, 0: 0.6},\n",
    "        3: {3: 0.2, 2: 0.2, 1: 0.5, 0: 0.1},\n",
    "    },\n",
    "    1: {\n",
    "        0: {0: 0.2, 1: 0.8},\n",
    "        1: {1: 0.2, 2: 0.2, 0: 0.6},\n",
    "        2: {3: 0.2, 2: 0.2, 1: 0.5, 0: 0.1},\n",
    "    },\n",
    "    2: {\n",
    "        0: {3: 0.2, 2: 0.2, 1: 0.6},\n",
    "        1: {3: 0.2, 2: 0.2, 1: 0.5, 0: 0.1},\n",
    "    },\n",
    "    3: {\n",
    "        0: {3: 0.2, 2: 0.2, 1: 0.5, 0: 0.1},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Costs c(i, a)\n",
    "c = {\n",
    "    (3, 0): 6,\n",
    "    (2, 1): 61,\n",
    "    (1, 2): 81,\n",
    "    (0, 3): 101,\n",
    "    (2, 0): 3.9+1.5,\n",
    "    (1, 1): 58.9+1.5,\n",
    "    (0, 2): 78.9+1.5,\n",
    "    (0, 0): 45,\n",
    "    (0, 1): 66.3+10.5,\n",
    "    (1, 0): 11.3+10.5,\n",
    "}\n",
    "# Initialize the LP problem\n",
    "lp_problem = pulp.LpProblem(\"Inventory_Optimization\", pulp.LpMinimize)\n",
    "\n",
    "# Decision variables z(i,a)\n",
    "z = pulp.LpVariable.dicts(\"z\", [(i, a) for i in states for a in actions[i]], lowBound=0)\n",
    "\n",
    "# Objective Function: Minimize cost\n",
    "lp_problem += pulp.lpSum(c[i, a] * z[i, a] for i in states for a in actions[i]), \"Total_Cost\"\n",
    "\n",
    "# Flow conservation constraint\n",
    "for j in states:\n",
    "    inflow = pulp.lpSum(\n",
    "        z[i, a] * P[i][a].get(j, 0)\n",
    "        for i in states\n",
    "        for a in actions[i]\n",
    "    )\n",
    "    outflow = pulp.lpSum(z[j, b] for b in actions[j])\n",
    "    lp_problem += inflow == outflow, f\"Flow_Conservation_{j}\"\n",
    "\n",
    "# Normalization constraint\n",
    "lp_problem += pulp.lpSum(z[i, a] for i in states for a in actions[i]) == 1, \"Normalization\"\n",
    "\n",
    "# Solve the LP\n",
    "lp_problem.solve()\n",
    "\n",
    "# Check the status\n",
    "print(f\"Status: {pulp.LpStatus[lp_problem.status]}\")\n",
    "\n",
    "# Optimal Decision Variable Values\n",
    "z_values = {(i, a): pulp.value(z[i, a]) for i in states for a in actions[i]}\n",
    "print(\"Optimal z(i,a) values:\")\n",
    "for key, value in z_values.items():\n",
    "    print(f\"z{key} = {value}\")\n",
    "\n",
    "# Compute the optimal policy f_{i,a}\n",
    "print(\"\\nOptimal Policy f_{i,a}:\")\n",
    "for i in states:\n",
    "    denominator = sum(z_values[i, b] for b in actions[i])\n",
    "    for a in actions[i]:\n",
    "        if denominator > 0:\n",
    "            f_ia = z_values[i, a] / denominator\n",
    "            print(f\"f({i},{a}) = {f_ia:.4f}\")\n",
    "        else:\n",
    "            print(f\"f({i},{a}) = 0.0000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Demand (items sold): 0.5000\n",
      "Revenue: 125.00\n",
      "Total Cost: 33.21\n",
      "Profit: 91.79\n"
     ]
    }
   ],
   "source": [
    "# Compute Expected Demand\n",
    "expected_demand = sum(a * z_values[i, a] for i in states for a in actions[i])\n",
    "print(f\"\\nExpected Demand (items sold): {expected_demand:.4f}\")\n",
    "\n",
    "# Selling price per item\n",
    "selling_price = 250\n",
    "\n",
    "# Compute Revenue and Profit\n",
    "revenue = expected_demand * selling_price\n",
    "total_cost = pulp.value(lp_problem.objective)  # Total cost from LP\n",
    "profit = revenue - total_cost\n",
    "\n",
    "print(f\"Revenue: {revenue:.2f}\")\n",
    "print(f\"Total Cost: {total_cost:.2f}\")\n",
    "print(f\"Profit: {profit:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
